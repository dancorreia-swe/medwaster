name: medwaster

services:
  # ============================================
  # Infrastructure Services
  # ============================================

  postgres:
    image: pgvector/pgvector:pg17-trixie
    container_name: medwaster-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-medwaster}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - medwaster-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:alpine
    container_name: medwaster-redis
    command: redis-server --appendonly yes
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - medwaster-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    container_name: medwaster-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - minio_data:/data
    networks:
      - medwaster-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

  localai:
    image: quay.io/go-skynet/local-ai:latest
    container_name: medwaster-localai
    profiles: ["ai"]
    environment:
      MODELS_PATH: /models
      THREADS: 4
      CONTEXT_SIZE: 512
      DEBUG: "false"
    command: ["--models-path", "/models"]
    ports:
      - "${LOCALAI_PORT:-8080}:8080"
    volumes:
      - ./localai/models:/models
    networks:
      - medwaster-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  model-loader:
    image: alpine:latest
    container_name: medwaster-model-loader
    profiles: ["ai"]
    command:
      - sh
      - -c
      - |
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo 'üì¶ LocalAI Model Loader'
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo ''

        set -euo pipefail

        CHAT_MODEL_PATH=/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
        EMBED_MODEL_PATH=/models/nomic-embed-text-v1.5.Q4_K_M.gguf
        WHISPER_MODEL_PATH=/models/ggml-small.en.bin

        # Ensure target directory exists
        mkdir -p /models

        # Ensure model config YAMLs exist (overwrite to keep in sync)
        echo 'üìù Syncing model configuration files...'
        cat > /models/mistral-7b-instruct-v0.2.Q4_K_M.yaml <<'EOF'
name: mistral-7b-instruct-v0.2.Q4_K_M
backend: llama
model: mistral-7b-instruct-v0.2.Q4_K_M.gguf
context_size: 8192
parameters:
  temperature: 0.7
  top_p: 0.95
  repeat_penalty: 1.1
template:
  chat_message: |-
    {{#each messages}}
    {{#if (eq this.role "system")}}[INST] {{this.content}} [/INST]{{/if}}
    {{#if (eq this.role "user")}}[INST] {{this.content}} [/INST]{{/if}}
    {{#if (eq this.role "assistant")}}{{this.content}}{{/if}}
    {{/each}}
EOF

        cat > /models/nomic-embed-text-v1.5.Q4_K_M.yaml <<'EOF'
name: nomic-embed-text-v1.5.Q4_K_M
backend: llama
model: nomic-embed-text-v1.5.Q4_K_M.gguf
parameters:
  embedding: true
EOF

        cat > /models/ggml-small.en.yaml <<'EOF'
name: ggml-small.en
backend: whisper
model: ggml-small.en.bin
parameters:
  language: pt-br
EOF
        echo '   ‚úÖ Chat model config (mistral-7b-instruct-v0.2.Q4_K_M.yaml)'
        echo '   ‚úÖ Embedding model config (nomic-embed-text-v1.5.Q4_K_M.yaml)'
        echo '   ‚úÖ Whisper model config (ggml-small.en.yaml)'
        echo ''

        # Check if models already exist
        echo 'üîç Checking for existing models...'
        ALL_EXIST=true

        if [ -f "$${CHAT_MODEL_PATH}" ]; then
          SIZE=$$(du -h "$${CHAT_MODEL_PATH}" | cut -f1)
          echo "   ‚úÖ Chat model found ($${SIZE})"
        else
          echo "   ‚è≥ Chat model missing"
          ALL_EXIST=false
        fi

        if [ -f "$${EMBED_MODEL_PATH}" ]; then
          SIZE=$$(du -h "$${EMBED_MODEL_PATH}" | cut -f1)
          echo "   ‚úÖ Embedding model found ($${SIZE})"
        else
          echo "   ‚è≥ Embedding model missing"
          ALL_EXIST=false
        fi

        if [ -f "$${WHISPER_MODEL_PATH}" ]; then
          SIZE=$$(du -h "$${WHISPER_MODEL_PATH}" | cut -f1)
          echo "   ‚úÖ Whisper model found ($${SIZE})"
        else
          echo "   ‚è≥ Whisper model missing"
          ALL_EXIST=false
        fi
        echo ''

        if [ "$${ALL_EXIST}" = "true" ]; then
          echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
          echo '‚úÖ All models already present - ready to use!'
          echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
          exit 0
        fi

        # Install curl (follow redirects reliably)
        echo 'üì¶ Installing download tools...'
        apk add --no-cache curl
        echo ''

        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo 'üì• Downloading Models'
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo ''

        # Download chat model (Mistral 7B Instruct, ~4.1GB Q4_K_M)
        if [ ! -f "$${CHAT_MODEL_PATH}" ]; then
          echo '‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê'
          echo '‚îÇ [1/3] Chat Model: Mistral 7B Instruct (Q4_K_M)     ‚îÇ'
          echo '‚îÇ       Expected size: ~4.1 GB                        ‚îÇ'
          echo '‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò'
          curl -L --fail --show-error --progress-bar -o "$${CHAT_MODEL_PATH}" \
            https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
            || { echo '‚ùå Chat model download failed'; exit 1; }
          SIZE=$$(du -h "$${CHAT_MODEL_PATH}" | cut -f1)
          echo "‚úÖ Downloaded successfully ($${SIZE})"
          echo ''
        fi

        # Download embedding model (~190MB Q4_K_M)
        if [ ! -f "$${EMBED_MODEL_PATH}" ]; then
          echo '‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê'
          echo '‚îÇ [2/3] Embedding Model: nomic-embed-text (Q4_K_M)   ‚îÇ'
          echo '‚îÇ       Expected size: ~190 MB                        ‚îÇ'
          echo '‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò'
          curl -L --fail --show-error --progress-bar -o "$${EMBED_MODEL_PATH}" \
            https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf \
            || { echo '‚ùå Embedding model download failed'; exit 1; }
          SIZE=$$(du -h "$${EMBED_MODEL_PATH}" | cut -f1)
          echo "‚úÖ Downloaded successfully ($${SIZE})"
          echo ''
        fi

        # Download whisper model (~244MB)
        if [ ! -f "$${WHISPER_MODEL_PATH}" ]; then
          echo '‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê'
          echo '‚îÇ [3/3] Whisper Model: ggml-small.en                 ‚îÇ'
          echo '‚îÇ       Expected size: ~244 MB                        ‚îÇ'
          echo '‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò'
          curl -L --fail --show-error --progress-bar -o "$${WHISPER_MODEL_PATH}" \
            https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.en.bin \
            || { echo '‚ùå Whisper model download failed'; exit 1; }
          SIZE=$$(du -h "$${WHISPER_MODEL_PATH}" | cut -f1)
          echo "‚úÖ Downloaded successfully ($${SIZE})"
          echo ''
        fi

        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo '‚úÖ Download Complete - Verifying Models'
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo ''
        echo 'Model files in /models:'
        ls -lh /models/*.{gguf,bin} 2>/dev/null || echo '  (no model files found)'
        echo ''
        echo 'Config files in /models:'
        ls -lh /models/*.yaml 2>/dev/null || echo '  (no config files found)'
        echo ''
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
        echo 'üéâ LocalAI is ready! Models loaded:'
        echo '   ‚Ä¢ mistral-7b-instruct-v0.2.Q4_K_M (chat)'
        echo '   ‚Ä¢ nomic-embed-text-v1.5.Q4_K_M (embeddings)'
        echo '   ‚Ä¢ ggml-small.en (whisper/transcription)'
        echo '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê'
    volumes:
      - ./localai/models:/models
    networks:
      - medwaster-network
    depends_on:
      localai:
        condition: service_healthy
    restart: on-failure

  # ============================================
  # Application Services
  # ============================================

  migrator:
    build:
      context: .
      dockerfile: apps/server/Dockerfile
    container_name: medwaster-migrator
    command: sh -c "bun run db:migrate && bun run db:seed"
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:password@postgres:5432/medwaster}
      NODE_ENV: ${NODE_ENV:-production}
      # Admin Setup (needed for seeding)
      ADMIN_EMAIL: ${ADMIN_EMAIL}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD}
      ADMIN_NAME: ${ADMIN_NAME}
    networks:
      - medwaster-network
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure

  server:
    build:
      context: .
      dockerfile: apps/server/Dockerfile
    container_name: medwaster-server
    environment:
      # Database
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:password@postgres:5432/medwaster}

      # Redis
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}

      # MinIO / S3
      S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY:-${MINIO_ROOT_USER:-minio}}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY:-${MINIO_ROOT_PASSWORD:-minio123}}
      S3_BUCKET: ${S3_BUCKET:-medwaster}
      S3_REGION: ${S3_REGION:-us-east-1}

      # Server
      PORT: ${SERVER_PORT:-4000}
      NODE_ENV: ${NODE_ENV:-production}

      # Auth
      BETTER_AUTH_SECRET: ${BETTER_AUTH_SECRET}
      BETTER_AUTH_URL: ${BETTER_AUTH_URL:-http://localhost:4000}
      CORS_ORIGIN: ${CORS_ORIGIN:-http://localhost:3000}

      # OAuth (optional)
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}

      # Email
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT:-587}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASS: ${SMTP_PASS}
      SMTP_FROM_ADDRESS: ${SMTP_FROM_ADDRESS:-noreply@medwaster.com}
      SMTP_FROM_NAME: ${SMTP_FROM_NAME:-MedWaster Learning}
      SMTP_SECURE: ${SMTP_SECURE:-false}

      # AI
      AI_PROVIDER: ${AI_PROVIDER:-openai}
      AI_CHAT_MODEL: ${AI_CHAT_MODEL:-gpt-4o}
      AI_EMBEDDING_MODEL: ${AI_EMBEDDING_MODEL:-text-embedding-3-small}
      AI_TRANSCRIPTION_MODEL: ${AI_TRANSCRIPTION_MODEL:-whisper-1}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LOCALAI_BASE_URL: ${LOCALAI_BASE_URL:-http://localai:8080/v1}
    ports:
      - "${SERVER_HOST_PORT:-4000}:4000"
    volumes:
      # Optional: Mount uploads directory if not using S3
      - ./uploads:/app/uploads
    networks:
      - medwaster-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      migrator:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/api/health"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped

  server-worker:
    build:
      context: .
      dockerfile: apps/server/Dockerfile
    container_name: medwaster-worker
    command: ["bun", "run", "src/workers/index.ts"]
    environment:
      # Same environment as server
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:password@postgres:5432/medwaster}
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY:-${MINIO_ROOT_USER:-minio}}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY:-${MINIO_ROOT_PASSWORD:-minio123}}
      S3_BUCKET: ${S3_BUCKET:-medwaster}
      S3_REGION: ${S3_REGION:-us-east-1}
      NODE_ENV: ${NODE_ENV:-production}
      AI_PROVIDER: ${AI_PROVIDER:-openai}
      AI_CHAT_MODEL: ${AI_CHAT_MODEL:-gpt-4o}
      AI_EMBEDDING_MODEL: ${AI_EMBEDDING_MODEL:-text-embedding-3-small}
      AI_TRANSCRIPTION_MODEL: ${AI_TRANSCRIPTION_MODEL:-whisper-1}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LOCALAI_BASE_URL: ${LOCALAI_BASE_URL:-http://localai:8080/v1}
    networks:
      - medwaster-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      server:
        condition: service_healthy
    restart: unless-stopped

  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    container_name: medwaster-web
    environment:
      VITE_SERVER_URL: ${VITE_SERVER_URL:-http://localhost:4000}
    ports:
      - "${WEB_HOST_PORT:-3000}:3000"
    networks:
      - medwaster-network
    depends_on:
      server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 3s
      start_period: 5s
      retries: 3
    restart: unless-stopped

  # ============================================
  # Reverse Proxy (Optional - Profile: proxy)
  # ============================================

  caddy:
    image: caddy:2-alpine
    container_name: medwaster-caddy
    profiles: ["proxy"]
    ports:
      - "${HTTP_PORT:-80}:80"
      - "${HTTPS_PORT:-443}:443"
      - "${CADDY_ADMIN_PORT:-2019}:2019"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
      - caddy_logs:/var/log/caddy
    environment:
      DOMAIN: ${DOMAIN:-localhost}
      LOCALAI_HOST: ${LOCALAI_HOST:-ai.localhost}
      LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL}
    networks:
      - medwaster-network
    depends_on:
      - server
      - web
    restart: unless-stopped

# ============================================
# Volumes
# ============================================

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  minio_data:
    driver: local
  caddy_data:
    driver: local
  caddy_config:
    driver: local
  caddy_logs:
    driver: local

# ============================================
# Networks
# ============================================

networks:
  medwaster-network:
    driver: bridge
